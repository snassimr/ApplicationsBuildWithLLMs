{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snassimr/ApplicationsBuildWithLLMs/blob/main/deoptima_generation_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY5ilTHy_WxF"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tv7US0CF_HyE"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ing0udi_eZe",
        "outputId": "3081547f-feba-4bc3-e734-966e79c8f55e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A7Xaez3cAtHH"
      },
      "outputs": [],
      "source": [
        "SYS_PROJECT_DIR  = '/content/gdrive/MyDrive/Colab Notebooks/deoptima'\n",
        "SYS_INPUT_DIR    = '/content/gdrive/MyDrive/Colab Notebooks/deoptima/input'\n",
        "SYS_MODELING_DIR = '/content/gdrive/MyDrive/Colab Notebooks/deoptima/modeling'\n",
        "SYS_TESTING_DIR = '/content/gdrive/MyDrive/Colab Notebooks/deoptima/testing'\n",
        "SYS_LLM_MODEL    = 'Mistral-7B-Instruct-v0.3'\n",
        "SYS_CHAT_IMAGES  = '/content/gdrive/MyDrive/Colab Notebooks/deoptima/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SvDFm5dAAih0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.options.mode.copy_on_write = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMO4ESpI5F6M"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmX0Qdub2tni",
        "outputId": "9fb762dd-4e6f-4dd6-e3cc-ddd6a4c9267d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "requirements = \"\"\"\n",
        "torch==2.3.1\n",
        "accelerate==0.32.1\n",
        "transformers==4.42.4\n",
        "vllm==0.5.4\n",
        "gradio==4.41.0\n",
        "nemoguardrails==0.9.1.1\n",
        "langchain_community==0.2.11\n",
        "langchain-openai==0.1.21\n",
        "\"\"\"\n",
        "requirements = \"\"\"\n",
        "gradio==4.41.0\n",
        "nemoguardrails==0.9.1.1\n",
        "langchain-openai==0.1.21\n",
        "\"\"\"\n",
        "\n",
        "requirements_path = os.path.join(SYS_PROJECT_DIR, 'requirements.txt')\n",
        "with open(requirements_path, 'w') as f:\n",
        "    f.write(requirements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DjyetbztnSlJ"
      },
      "outputs": [],
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JIMNva_V5IrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a072623-e4e6-4c64-e553-98dfdcb46b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'langchain-openai' candidate (version 0.1.21 at https://files.pythonhosted.org/packages/e5/0c/7c752ba4a6fa8719f9f8cf8e22f0e2c8bbcdac5f8a580dd12a1496cd5031/langchain_openai-0.1.21-py3-none-any.whl (from https://pypi.org/simple/langchain-openai/) (requires-python:<4.0,>=3.8.1))\n",
            "Reason for being yanked: Regression. AzureChatOpenAI json-mode broken. Fixed in 0.1.22.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyStemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "requirements_path = os.path.join(SYS_PROJECT_DIR, 'requirements.txt')\n",
        "!pip install -q -r '{requirements_path}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gr_5DHGsMvOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49b572b-e0d1-43d4-cd01-fe306aa0169a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:122: RuntimeWarning: The _imaging extension was built for another version of Pillow or PIL:\n",
            "Core version: 9.4.0\n",
            "Pillow version: 10.4.0\n",
            "  warnings.warn(str(v), RuntimeWarning)\n",
            "[autoreload of PIL.Image failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
            "    module = reload(module)\n",
            "  File \"/usr/lib/python3.10/imp.py\", line 315, in reload\n",
            "    return importlib.reload(module)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 108, in <module>\n",
            "    raise ImportError(msg)\n",
            "ImportError: The _imaging extension was built for another version of Pillow or PIL:\n",
            "Core version: 9.4.0\n",
            "Pillow version: 10.4.0\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "deoptima_path = os.path.join(SYS_PROJECT_DIR)\n",
        "if deoptima_path not in sys.path:\n",
        "    sys.path.append(deoptima_path)\n",
        "\n",
        "from deoptima.utils import set_logger\n",
        "from deoptima.utils import dictionary_to_string\n",
        "from deoptima.utils import conversation_to_string\n",
        "from deoptima.utils import process_state_to_string\n",
        "\n",
        "deoptima_logger = set_logger(log_mode = 'S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rnPo-noJ_6hd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np_bdDVx0i3W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok4HsW9nwQg9"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKxO4SGTWu2f"
      },
      "source": [
        "## Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hmq55G1gV195"
      },
      "outputs": [],
      "source": [
        "scenario_code = \"S0\"\n",
        "# scenario_code = \"S1\"\n",
        "n_version = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7uBjIxx7JMra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906f749d-c19b-4d75-8aa9-1d0dfb004ae7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abc_1_1.json']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "conversation_output_folder = os.path.join(SYS_TESTING_DIR, \"output\", scenario_code)\n",
        "conversation_files = [f for f in os.listdir(conversation_output_folder) if os.path.isfile(os.path.join(conversation_output_folder, f))]\n",
        "conversation_files = [file for file in conversation_files if file.split(\"_\")[1] == str(n_version)]\n",
        "conversation_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yn3Cbvy7CXq0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "def load_from_json(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "conversation_idx = 0\n",
        "conversation = load_from_json(os.path.join(conversation_output_folder, conversation_files[conversation_idx]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "from deoptima.utils import format_conversation\n",
        "\n",
        "html_content = format_conversation(conversation)\n",
        "display(HTML(html_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "L0mJ6qs6rZBB",
        "outputId": "42d621b2-ddda-4bda-bad9-4673cb1dc1bd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-family: Arial, sans-serif;'><p><strong style='color: #2ecc71;'>user:</strong> I'm ready to start the decision-making process.</p><p><strong style='color: #3498db;'>assistant:</strong> Great! Let's start by defining the problem you're trying to solve. What is the goal you want to achieve? Please provide a brief description of the problem. After that, we can move on to the options you are considering and the criteria that are important to you.</p><p><strong style='color: #2ecc71;'>user:</strong> I want to find the best vacation destination for my family.</p><p><strong style='color: #3498db;'>assistant:</strong> Now that we have defined the problem, let's move on to the options you are considering for your vacation destination. Please provide a list of possible vacation destinations that you are thinking about. After that, we will identify the criteria that are important to you when selecting the best destination.</p><p><strong style='color: #2ecc71;'>user:</strong> I'm thinking about going to the Bahamas, Cancun, or New York.</p><p><strong style='color: #3498db;'>assistant:</strong> Now that we have your options listed, let's identify the criteria that are important to you when selecting the best vacation destination. Please provide a list of criteria that you care about, such as price, climate, activities, etc.</p><p><strong style='color: #2ecc71;'>user:</strong> I care about price, climate, and activities.</p><p><strong style='color: #3498db;'>assistant:</strong> Now that we have defined the problem, options, and criteria, let's move on to comparing the criteria. Please provide your comparisons for each unique pair of criteria using the format: '<criterion_1> is <criteria_rating> to/than <criterion_2>'. For example, you could say 'Price is Moderately More Important than Climate'.</p><p><strong style='color: #2ecc71;'>user:</strong> Climate is Very Strongly More Important than Activities</p><p><strong style='color: #3498db;'>assistant:</strong> Thank you for your comparison! Now, please provide the remaining comparisons for the other pairs of criteria. You need to compare 'Price' with 'Climate' and 'Price' with 'Activities'. Use the format '<criterion_1> is <criteria_rating> to/than <criterion_2>'.</p><p><strong style='color: #2ecc71;'>user:</strong> Price is Moderately More Important than Climate</p><p><strong style='color: #3498db;'>assistant:</strong> Thank you for your comparisons! Now, please provide the final comparison for the last pair of criteria: 'Price' and 'Activities'. Use the format '<criterion_1> is <criteria_rating> to/than <criterion_2>'.</p><p><strong style='color: #2ecc71;'>user:</strong> Price is Strongly More Important than Activities</p><p><strong style='color: #3498db;'>assistant:</strong> All basic definitions and criteria comparisons are set. You can now proceed to Options Evaluation.</p><p><strong style='color: #2ecc71;'>user:</strong> Please evaluate the options based on Price.</p></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV3MAUgqCgtz"
      },
      "source": [
        "## get_evaluation_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "jOfTKgVIJW85"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "deoptima_path = os.path.join(SYS_PROJECT_DIR)\n",
        "if deoptima_path not in sys.path:\n",
        "    sys.path.append(deoptima_path)\n",
        "\n",
        "from deoptima.utils import conversation_to_string\n",
        "from deoptima.utils import conversation_to_string_numbered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "R_AEBzn8JW9B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "with open(os.path.join(deoptima_path, 'deoptima', 'generation_evaluation_data.yaml'), 'r') as file:\n",
        "    generation_evaluation_data = yaml.safe_load(file)\n",
        "\n",
        "with open(os.path.join(deoptima_path, 'deoptima', 'generation_data.yaml'), 'r') as file:\n",
        "    generation_data = yaml.safe_load(file)\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "def load_from_json(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "generation_learning_examples = load_from_json(os.path.join(os.path.join(SYS_PROJECT_DIR, \"deoptima\", \"generation_learning_examples.json\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "E1nPu70YJW9C"
      },
      "outputs": [],
      "source": [
        "criteria_rating_list = generation_data['infos']['criteria_rating_list']\n",
        "options_criteria_rating_list = generation_data['infos']['options_criteria_rating_list']\n",
        "process_technical_explanation = generation_data['infos']['process_technical_explanation']\n",
        "process_technical_explanation = process_technical_explanation.format(\n",
        "    criteria_rating_list=criteria_rating_list)\n",
        "\n",
        "get_evaluation_data_prompt = generation_evaluation_data['prompts']['get_evaluation_data']['prompt']\n",
        "get_evaluation_data_prompt = get_evaluation_data_prompt.format(\n",
        "                                                           conversation = conversation_to_string_numbered(conversation),\n",
        "                                                           process_technical_explanation=process_technical_explanation,\n",
        "                                                           Assistant_specificity_examples = generation_learning_examples['Assistant_specificity'],\n",
        "                                                           Assistant_technicality_examples = generation_learning_examples['Assistant_technicality'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "PEeSv9XLvwYU"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import Markdown\n",
        "# display(Markdown(f\"{get_evaluation_data_prompt}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "OSyd7AkyJW9C"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Literal, List, Dict, Tuple\n",
        "\n",
        "def get_evaluation_data(prompt : str) -> Tuple[Dict[Literal['assistant'], str],List[str]]:\n",
        "  import os\n",
        "  from langchain_openai import ChatOpenAI\n",
        "  from pydantic import BaseModel, Field\n",
        "  from typing import List, Literal\n",
        "  from langchain_core.prompts import ChatPromptTemplate\n",
        "  from langchain_core.prompts import PromptTemplate\n",
        "  from langchain_core.output_parsers import StrOutputParser\n",
        "  from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "  from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "  model = ChatOpenAI(model=\"gpt-4o\", temperature = 0.0, top_p = 1.0)\n",
        "\n",
        "  output_parser = StrOutputParser()\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "    template=prompt,\n",
        "  )\n",
        "\n",
        "  chain = prompt | model | output_parser\n",
        "\n",
        "  generation = chain.invoke({'prompt' : prompt})\n",
        "\n",
        "  print(generation)\n",
        "\n",
        "  import json\n",
        "\n",
        "  start_string = \"```json\" ; end_string = \"```\"\n",
        "  start_index = generation.find(start_string) + len(start_string)\n",
        "  end_index = generation.rfind(end_string) - 1\n",
        "\n",
        "  if start_index != -1 and end_index != -1:\n",
        "      json_string = generation[start_index:end_index]\n",
        "      response_data = json.loads(json_string)\n",
        "  else:\n",
        "      print(\"Start or end string not found\")\n",
        "\n",
        "  return response_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6vEHmiatJW9C",
        "outputId": "7ea08d8f-7115-46c5-8b3c-c3b8652adfca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO     : 2024-09-05 11:45:07,176 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"metric\": \"Completeness\",\n",
            "  \"score\": 1\n",
            "}\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "\n",
            "- **Metric: Completeness**\n",
            "  - **Score = 1**: The last assistant message contains information about the possibility to proceed to Step 3 or Options Evaluation. The last message is: \"All basic definitions and criteria comparisons are set. You can now proceed to Options Evaluation.\"\n",
            "  - **Score = 0**: This condition is not met because the last assistant message does contain information about proceeding to Step 3.\n",
            "  - **Score = -1**: This condition is not met because the assistant does provide information about proceeding to Step 3 in the last message.\n",
            "\n",
            "### Smoothness Calculation:\n",
            "\n",
            "1. **Message 0 (user)**: \"I'm ready to start the decision-making process.\"\n",
            "   - **Smooth**: Yes, it initiates the conversation.\n",
            "2. **Message 1 (assistant)**: \"Great! Let's start by defining the problem you're trying to solve. What is the goal you want to achieve? Please provide a brief description of the problem. After that, we can move on to the options you are considering and the criteria that are important to you.\"\n",
            "   - **Smooth**: Yes, it logically follows the user's initiation.\n",
            "3. **Message 2 (user)**: \"I want to find the best vacation destination for my family.\"\n",
            "   - **Smooth**: Yes, it provides the problem definition as requested.\n",
            "4. **Message 3 (assistant)**: \"Now that we have defined the problem, let's move on to the options you are considering for your vacation destination. Please provide a list of possible vacation destinations that you are thinking about. After that, we will identify the criteria that are important to you when selecting the best destination.\"\n",
            "   - **Smooth**: Yes, it logically follows the user's problem definition.\n",
            "5. **Message 4 (user)**: \"I'm thinking about going to the Bahamas, Cancun, or New York.\"\n",
            "   - **Smooth**: Yes, it provides the options as requested.\n",
            "6. **Message 5 (assistant)**: \"Now that we have your options listed, let's identify the criteria that are important to you when selecting the best vacation destination. Please provide a list of criteria that you care about, such as price, climate, activities, etc.\"\n",
            "   - **Smooth**: Yes, it logically follows the user's options list.\n",
            "7. **Message 6 (user)**: \"I care about price, climate, and activities.\"\n",
            "   - **Smooth**: Yes, it provides the criteria as requested.\n",
            "8. **Message 7 (assistant)**: \"Now that we have defined the problem, options, and criteria, let's move on to comparing the criteria. Please provide your comparisons for each unique pair of criteria using the format: '<criterion_1> is <criteria_rating> to/than <criterion_2>'. For example, you could say 'Price is Moderately More Important than Climate'.\"\n",
            "   - **Smooth**: Yes, it logically follows the user's criteria list.\n",
            "9. **Message 8 (user)**: \"Climate is Very Strongly More Important than Activities.\"\n",
            "   - **Smooth**: Yes, it provides a criteria comparison as requested.\n",
            "10. **Message 9 (assistant)**: \"Thank you for your comparison! Now, please provide the remaining comparisons for the other pairs of criteria. You need to compare 'Price' with 'Climate' and 'Price' with 'Activities'. Use the format '<criterion_1> is <criteria_rating> to/than <criterion_2>'.\"\n",
            "    - **Smooth**: Yes, it logically follows the user's criteria comparison.\n",
            "11. **Message 10 (user)**: \"Price is Moderately More Important than Climate.\"\n",
            "    - **Smooth**: Yes, it provides a criteria comparison as requested.\n",
            "12. **Message 11 (assistant)**: \"Thank you for your comparisons! Now, please provide the final comparison for the last pair of criteria: 'Price' and 'Activities'. Use the format '<criterion_1> is <criteria_rating> to/than <criterion_2>'.\"\n",
            "    - **Smooth**: Yes, it logically follows the user's criteria comparison.\n",
            "13. **Message 12 (user)**: \"Price is Strongly More Important than Activities.\"\n",
            "    - **Smooth**: Yes, it provides a criteria comparison as requested.\n",
            "14. **Message 13 (assistant)**: \"All basic definitions and criteria comparisons are set. You can now proceed to Options Evaluation.\"\n",
            "    - **Smooth**: Yes, it logically follows the user's criteria comparison.\n",
            "\n",
            "- **Smoothness Score**: (14 smooth messages / 14 total messages) * 100 = 100%\n",
            "\n",
            "### User Specificity Calculation:\n",
            "\n",
            "1. **Message 0 (user)**: \"I'm ready to start the decision-making process.\"\n",
            "   - **Specific**: No, it is a general statement.\n",
            "2. **Message 2 (user)**: \"I want to find the best vacation destination for my family.\"\n",
            "   - **Specific**: Yes, it provides a clear problem definition.\n",
            "3. **Message 4 (user)**: \"I'm thinking about going to the Bahamas, Cancun, or New York.\"\n",
            "   - **Specific**: Yes, it provides specific options.\n",
            "4. **Message 6 (user)**: \"I care about price, climate, and activities.\"\n",
            "   - **Specific**: Yes, it provides specific criteria.\n",
            "5. **Message 8 (user)**: \"Climate is Very Strongly More Important than Activities.\"\n",
            "   - **Specific**: Yes, it provides a specific criteria comparison.\n",
            "6. **Message 10 (user)**: \"Price is Moderately More Important than Climate.\"\n",
            "   - **Specific**: Yes, it provides a specific criteria comparison.\n",
            "7. **Message 12 (user)**: \"Price is Strongly More Important than Activities.\"\n",
            "   - **Specific**: Yes, it provides a specific criteria comparison.\n",
            "\n",
            "- **User Specificity Score**: (6 specific messages / 7 total user messages) * 100 = 85.71%\n",
            "\n",
            "### Assistant Specificity Calculation:\n",
            "\n",
            "1. **Message 1 (assistant)**: \"Great! Let's start by defining the problem you're trying to solve. What is the goal you want to achieve? Please provide a brief description of the problem. After that, we can move on to the options you are considering and the criteria that are important to you.\"\n",
            "   - **Specific**: Yes, it provides clear instructions on what to do next.\n",
            "2. **Message 3 (assistant)**: \"Now that we have defined the problem, let's move on to the options you are considering for your vacation destination. Please provide a list of possible vacation destinations that you are thinking about. After that, we will identify the criteria that are important to you when selecting the best destination.\"\n",
            "   - **Specific**: Yes, it provides clear instructions on what to do next.\n",
            "3. **Message 5 (assistant)**: \"Now that we have your options listed, let's identify the criteria that are important to you when selecting the best vacation destination. Please provide a list of criteria that you care about, such as price, climate, activities, etc.\"\n",
            "   - **Specific**: Yes, it provides clear instructions on what to do next.\n",
            "4. **Message 7 (assistant)**: \"Now that we have defined the problem, options, and criteria, let's move on to comparing the criteria. Please provide your comparisons for each unique pair of criteria using the format: '<criterion_1> is <criteria_rating> to/than <criterion_2>'. For example, you could say 'Price is Moderately More Important than Climate'.\"\n",
            "   - **Specific**: Yes, it provides clear instructions on what to do next.\n",
            "5. **Message 9 (assistant)**: \"Thank you for your comparison! Now, please provide the remaining comparisons for the other pairs of criteria. You need to compare 'Price' with 'Climate' and 'Price' with 'Activities'. Use the format '<criterion_1> is <criteria_rating> to/than <criterion_2>'.\"\n",
            "   - **Specific**: Yes, it provides clear instructions on what to do next.\n",
            "6. **Message 11 (assistant)**: \"Thank you for your comparisons! Now, please provide the final comparison for the last pair of criteria: 'Price' and 'Activities'. Use the format '<criterion_1> is <criteria_rating> to/than <criterion_2>'.\"\n",
            "   - **Specific**: Yes, it provides clear instructions on what to do next.\n",
            "7. **Message 13 (assistant)**: \"All basic definitions and criteria comparisons are set. You can now proceed to Options Evaluation.\"\n",
            "   - **Specific**: Yes, it provides clear instructions on what to do next.\n",
            "\n",
            "- **Assistant Specificity Score**: (7 specific messages / 7 total assistant messages) * 100 = 100%\n",
            "\n",
            "### Assistant Technicality Calculation:\n",
            "\n",
            "1. **Message 1 (assistant)**: \"Great! Let's start by defining the problem you're trying to solve. What is the goal you want to achieve? Please provide a brief description of the problem. After that, we can move on to the options you are considering and the criteria that are important to you.\"\n",
            "   - **Technical**: No, it does not contain technical information.\n",
            "2. **Message 3 (assistant)**: \"Now that we have defined the problem, let's move on to the options you are considering for your vacation destination. Please provide a list of possible vacation destinations that you are thinking about. After that, we will identify the criteria that are important to you when selecting the best destination.\"\n",
            "   - **Technical**: No, it does not contain technical information.\n",
            "3. **Message 5 (assistant)**: \"Now that we have your options listed, let's identify the criteria that are important to you when selecting the best vacation destination. Please provide a list of criteria that you care about, such as price, climate, activities, etc.\"\n",
            "   - **Technical**: No, it does not contain technical information.\n",
            "4. **Message 7 (assistant)**: \"Now that we have defined the problem, options, and criteria, let's move on to comparing the criteria. Please provide your comparisons for each unique pair of criteria using the format: '<criterion_1> is <criteria_rating> to/than <criterion_2>'. For example, you could say 'Price is Moderately More Important than Climate'.\"\n",
            "   - **Technical**: Yes, it contains technical information about the format.\n",
            "5. **Message 9 (assistant)**: \"Thank you for your comparison! Now, please provide the remaining comparisons for the other pairs of criteria. You need to compare 'Price' with 'Climate' and 'Price' with 'Activities'. Use the format '<criterion_1> is <criteria_rating> to/than <criterion_2>'.\"\n",
            "   - **Technical**: Yes, it contains technical information about the format.\n",
            "6. **Message 11 (assistant)**: \"Thank you for your comparisons! Now, please provide the final comparison for the last pair of criteria: 'Price' and 'Activities'. Use the format '<criterion_1> is <criteria_rating> to/than <criterion_2>'.\"\n",
            "   - **Technical**: Yes, it contains technical information about the format.\n",
            "7. **Message 13 (assistant)**: \"All basic definitions and criteria comparisons are set. You can now proceed to Options Evaluation.\"\n",
            "   - **Technical**: No, it does not contain technical information.\n",
            "\n",
            "- **Assistant Technicality Score**: (4 not technical messages / 7 total assistant messages) * 100 = 57.14%\n",
            "\n",
            "### Final Output:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"metric\": \"Completeness\",\n",
            "  \"score\": 1\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Extra data: line 6 column 1 (char 46)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-6dc2c3cb8a4b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_evaluation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_evaluation_data_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-8b64128f4674>\u001b[0m in \u001b[0;36mget_evaluation_data\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mend_index\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mjson_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start or end string not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 6 column 1 (char 46)"
          ]
        }
      ],
      "source": [
        "response_data = get_evaluation_data(get_evaluation_data_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnoqFe3mJvFm",
        "outputId": "b079f95a-a8e9-4df0-e0c5-7d05b92da225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO     : 2024-09-05 00:03:50,072 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO     : 2024-09-05 00:03:50,077 : get_evaluation_data finished: [{'metric': 'Completeness', 'score': 1}, {'metric': 'Smoothness', 'score': 100}, {'metric': 'User_specificity', 'score': 100}, {'metric': 'Assistant_specificity', 'score': 83.33}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "[\n",
            "  {\n",
            "    \"metric\": \"Completeness\",\n",
            "    \"score\": 1\n",
            "  },\n",
            "  {\n",
            "    \"metric\": \"Smoothness\",\n",
            "    \"score\": 100\n",
            "  },\n",
            "  {\n",
            "    \"metric\": \"User_specificity\",\n",
            "    \"score\": 100\n",
            "  },\n",
            "  {\n",
            "    \"metric\": \"Assistant_specificity\",\n",
            "    \"score\": 83.33\n",
            "  }\n",
            "]\n",
            "```\n",
            "Function runtime: 1115.00 msec\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "  response_data = get_evaluation_data(get_evaluation_data_prompt)\n",
        "except Exception as e:\n",
        "  deoptima_logger.error(f\"Error in {get_evaluation_data.__name__}: {str(e)}\")\n",
        "else:\n",
        "  deoptima_logger.info(f\"{get_evaluation_data.__name__} finished: {str(response_data)}\")\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "runtime_ms = (end_time - start_time) * 1000\n",
        "print(f\"Function runtime: {runtime_ms:.2f} msec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2WOfXPxAoNN"
      },
      "source": [
        "# generate_evaluation_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "-4nwx6B-YUtg"
      },
      "outputs": [],
      "source": [
        "# scenario_code = \"S0\"\n",
        "scenario_code = \"S1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "WRl70SMNK1fV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "conversation_output_folder = os.path.join(SYS_TESTING_DIR, \"output\", scenario_code)\n",
        "conversation_files = [f for f in os.listdir(conversation_output_folder) if os.path.isfile(os.path.join(conversation_output_folder, f))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33zyAhWaYTVD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "def load_from_json(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "conversation_idx = 1\n",
        "conversation = load_from_json(os.path.join(conversation_output_folder, conversation_files[conversation_idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2kZzRPGRXig"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import display, HTML\n",
        "# from deoptima.utils import format_conversation\n",
        "\n",
        "# html_content = format_conversation(conversation)\n",
        "# display(HTML(html_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxF71M7bBASJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "f0b00ce4-f7c8-44bb-c5a9-c37bc3e536c1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/Colab Notebooks/deoptima/deoptima/generation_data.yaml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-cf9f0522f419>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeoptima_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deoptima'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'generation_data.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mgeneration_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/Colab Notebooks/deoptima/deoptima/generation_data.yaml'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "deoptima_path = os.path.join(SYS_PROJECT_DIR)\n",
        "if deoptima_path not in sys.path:\n",
        "    sys.path.append(deoptima_path)\n",
        "\n",
        "from deoptima.utils import conversation_to_string\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "with open(os.path.join(deoptima_path, 'deoptima', 'generation_evaluation_data.yaml'), 'r') as file:\n",
        "    generation_evaluation_data = yaml.safe_load(file)\n",
        "\n",
        "def generate_evaluation_data(conversation: List[Dict[str, str]]):\n",
        "\n",
        "  ### get_evaluation_data\n",
        "  get_evaluation_data_prompt = generation_evaluation_data['prompts']['get_evaluation_data']['prompt']\n",
        "  get_evaluation_data_prompt = get_evaluation_data_prompt.format(process_technical_explanation=process_technical_explanation,\n",
        "                                                             conversation=conversation_to_string(conversation),\n",
        "                                                             process_state=process_state_to_string(process_state))\n",
        "\n",
        "  try:\n",
        "    response_data = get_evaluation_data(get_evaluation_data_prompt)\n",
        "    response_finished = response_data['step_finished']\n",
        "  except Exception as e:\n",
        "    deoptima_logger.error(f\"Error in {get_evaluation_data.__name__}: {str(e)}\")\n",
        "  else:\n",
        "    deoptima_logger.info(f\"{get_evaluation_data.__name__} finished: {response_finished}\")\n",
        "\n",
        "  return response_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO0FNza8pCtV"
      },
      "source": [
        "# Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNKMDkTg31fb",
        "outputId": "8299f486-62ef-4b6e-dda4-7020d2891969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO     : 2024-09-04 14:27:53,780 : generate_model_response started.\n",
            "INFO     : 2024-09-04 14:27:57,230 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO     : 2024-09-04 14:27:57,235 : get_process_state finished: \n",
            "Problem: select the best laptop\n",
            "\n",
            "Options:\n",
            "- Laptop model 1\n",
            "- Laptop model 2\n",
            "- Laptop model 3\n",
            "\n",
            "Criteria:\n",
            "- Price\n",
            "- Performance\n",
            "- Battery Life\n",
            "\n",
            "Criteria Comparisons:\n",
            "- Price is Moderately More Important than Performance\n",
            "- Battery Life is Strongly More Important than Price\n",
            "\n",
            "INFO     : 2024-09-04 14:27:59,355 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO     : 2024-09-04 14:27:59,361 : get_response_data finished: No\n",
            "INFO     : 2024-09-04 14:27:59,362 : assistant_message: Thank you for the comparisons! Now, please provide the next criteria comparison.\n",
            "INFO     : 2024-09-04 14:27:59,363 : user_message : Battery Life is Equally Important to Performance.\n",
            "INFO     : 2024-09-04 14:27:59,365 : generate_model_response finished.\n",
            "INFO     : 2024-09-04 14:27:59,366 : generate_model_response started.\n",
            "INFO     : 2024-09-04 14:28:01,529 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO     : 2024-09-04 14:28:01,535 : get_process_state finished: \n",
            "Problem: select the best laptop\n",
            "\n",
            "Options:\n",
            "- Laptop model 1\n",
            "- Laptop model 2\n",
            "- Laptop model 3\n",
            "\n",
            "Criteria:\n",
            "- Price\n",
            "- Performance\n",
            "- Battery Life\n",
            "\n",
            "Criteria Comparisons:\n",
            "- Price is Moderately More Important than Performance\n",
            "- Battery Life is Strongly More Important than Price\n",
            "- Battery Life is Equally Important than Performance\n",
            "\n",
            "INFO     : 2024-09-04 14:28:04,662 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO     : 2024-09-04 14:28:04,666 : get_response_data finished: No\n",
            "INFO     : 2024-09-04 14:28:04,667 : assistant_message: Thank you for the comparisons! Now we need to complete the criteria comparisons. Please provide the last comparison between Performance and Battery Life.\n",
            "INFO     : 2024-09-04 14:28:04,668 : user_message : Performance is Equally Important to Battery Life.\n",
            "INFO     : 2024-09-04 14:28:04,669 : generate_model_response finished.\n",
            "INFO     : 2024-09-04 14:28:04,670 : generate_model_response started.\n",
            "INFO     : 2024-09-04 14:28:06,604 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO     : 2024-09-04 14:28:06,609 : get_process_state finished: \n",
            "Problem: select the best laptop\n",
            "\n",
            "Options:\n",
            "- Laptop model 1\n",
            "- Laptop model 2\n",
            "- Laptop model 3\n",
            "\n",
            "Criteria:\n",
            "- Price\n",
            "- Performance\n",
            "- Battery Life\n",
            "\n",
            "Criteria Comparisons:\n",
            "- Price is Moderately More Important than Performance\n",
            "- Battery Life is Strongly More Important than Price\n",
            "- Battery Life is Equally Important than Performance\n",
            "\n",
            "INFO     : 2024-09-04 14:28:08,782 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO     : 2024-09-04 14:28:08,787 : get_response_data finished: Yes\n",
            "INFO     : 2024-09-04 14:28:10,603 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO     : 2024-09-04 14:28:10,609 : get_process_state finished: \n",
            "Problem: select the best laptop\n",
            "\n",
            "Options:\n",
            "- Laptop model 1\n",
            "- Laptop model 2\n",
            "- Laptop model 3\n",
            "\n",
            "Criteria:\n",
            "- Price\n",
            "- Performance\n",
            "- Battery Life\n",
            "\n",
            "Criteria Comparisons:\n",
            "- Price is Moderately More Important than Performance\n",
            "- Battery Life is Strongly More Important than Price\n",
            "- Battery Life is Equally Important than Performance\n",
            "\n",
            "INFO     : 2024-09-04 14:28:21,873 : HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO     : 2024-09-04 14:28:21,878 : get_validation_data finished: \n",
            "Validity Checks: {'name': 'Problem size', 'sbsc_explanation': 'The number of options (n_o) is 3 and the number of criteria (n_c) is 3. Therefore, n_o * n_c = 3 * 3 = 9, which is less than 21. This condition holds true.', 'sbss_explanation': '', 'status': 'Pass'}, {'name': 'Criteria comparison statements validity', 'sbsc_explanation': \"The criteria comparison statements provided are: 'Price is Moderately More Important than Performance', 'Battery Life is Strongly More Important than Price', and 'Battery Life is Equally Important than Performance'. All these statements are valid according to the defined criteria rating list.\", 'sbss_explanation': '', 'status': 'Pass'}, {'name': 'Number of criteria comparison statements', 'sbsc_explanation': 'The number of criteria comparison statements (n_cc) provided is 3. The formula for the expected number of criteria comparison statements is n_c * (n_c - 1) / 2, which for n_c = 3 gives us 3 * (3 - 1) / 2 = 3. This condition holds true.', 'sbss_explanation': '', 'status': 'Pass'}\n",
            "Response Data: {'assistant_message': 'All basic definitions and criteria comparisons are set. You can now proceed to the Options Evaluation.', 'user_examples': ['I prefer Laptop model 1 over Laptop model 2.', 'I think Laptop model 3 is better than Laptop model 1.', 'Laptop model 2 is my top choice compared to Laptop model 3.']}\n",
            " all_validity_checks_passed : True\n",
            "INFO     : 2024-09-04 14:28:21,879 : assistant_message: All basic definitions and criteria comparisons are set. You can now proceed to the Options Evaluation.\n",
            "INFO     : 2024-09-04 14:28:21,880 : user_message : I prefer Laptop model 1 over Laptop model 2.\n",
            "INFO     : 2024-09-04 14:28:21,882 : generate_model_response finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function runtime: 28474.07 msec\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# scenario_code = \"S0\"\n",
        "scenario_code = \"S1\"\n",
        "\n",
        "conversation_extra_length = 10\n",
        "start_conversation_length = len(conversation)\n",
        "run_cnt = 1 ; run_limit = 10\n",
        "\n",
        "while not(stop_criterion):\n",
        "  deoptima_logger.info(f\"{generate_model_response.__name__} started.\")\n",
        "  try:\n",
        "    response_finished, all_validity_checks_passed, assistant_message,  user_examples = generate_model_response(conversation)\n",
        "    import random\n",
        "    user_message = random.choice(user_examples)\n",
        "  except Exception as e:\n",
        "    deoptima_logger.error(f\"Error in {generate_model_response.__name__}: {str(e)}\")\n",
        "  else:\n",
        "    deoptima_logger.info(f\"assistant_message: {assistant_message}\")\n",
        "    deoptima_logger.info(f\"user_message : {user_message}\")\n",
        "    deoptima_logger.info(f\"{generate_model_response.__name__} finished.\")\n",
        "\n",
        "    conversation.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_message})\n",
        "    run_cnt += 1\n",
        "\n",
        "  stop_criterion = (all_validity_checks_passed) or (len(conversation) > (start_conversation_length + conversation_extra_length)) or run_cnt > run_limit\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "conversation_output_folder = os.path.join(SYS_TESTING_DIR, \"output\", scenario_code)\n",
        "\n",
        "def save_to_json(obj, filepath):\n",
        "    with open(filepath, 'w') as file:\n",
        "        json.dump(obj, file, indent=4)\n",
        "\n",
        "save_to_json(conversation, os.path.join(conversation_output_folder, conversation_files[conversation_idx]))\n",
        "\n",
        "end_time = time.time()\n",
        "runtime_ms = (end_time - start_time) * 1000\n",
        "print(f\"Function runtime: {runtime_ms:.2f} msec\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "bKrgSR_q-HYe",
        "outputId": "88973440-a327-47e1-c602-52c1d00f735c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-family: Arial, sans-serif;'><p><strong style='color: #2ecc71;'>user:</strong> I'm ready to start the decision-making process.</p><p><strong style='color: #3498db;'>assistant:</strong> Great! Let's begin with Step 1. Please provide the problem you are trying to solve.</p><p><strong style='color: #2ecc71;'>user:</strong> I want to select the best laptop.</p><p><strong style='color: #3498db;'>assistant:</strong> Got it! Now, please provide a list of options or laptop models you are considering.</p><p><strong style='color: #2ecc71;'>user:</strong> I'm looking at Laptop model 1, Laptop model 2, and Laptop model 3.</p><p><strong style='color: #3498db;'>assistant:</strong> Perfect! Now, please list the criteria that are important to you when selecting a laptop.</p><p><strong style='color: #2ecc71;'>user:</strong> I care about Price, Performance, and Battery Life.</p><p><strong style='color: #3498db;'>assistant:</strong> Thank you! Now we have the problem, options, and criteria defined. Let's move to Step 2. Please compare each unique pair of criteria. For example, you can say 'Price is Equally Important to Performance'.</p><p><strong style='color: #2ecc71;'>user:</strong> Price is Moderately More Important than Performance.</p><p><strong style='color: #3498db;'>assistant:</strong> Great! Now, please provide another comparison.</p><p><strong style='color: #2ecc71;'>user:</strong> Battery Life is Strongly More Important than Price.</p><p><strong style='color: #3498db;'>assistant:</strong> Thank you for the comparisons! Now, please provide the next criteria comparison.</p><p><strong style='color: #2ecc71;'>user:</strong> Battery Life is Equally Important to Performance.</p><p><strong style='color: #3498db;'>assistant:</strong> Thank you for the comparisons! Now we need to complete the criteria comparisons. Please provide the last comparison between Performance and Battery Life.</p><p><strong style='color: #2ecc71;'>user:</strong> Performance is Equally Important to Battery Life.</p><p><strong style='color: #3498db;'>assistant:</strong> All basic definitions and criteria comparisons are set. You can now proceed to the Options Evaluation.</p><p><strong style='color: #2ecc71;'>user:</strong> I prefer Laptop model 1 over Laptop model 2.</p></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "from deoptima.utils import format_conversation\n",
        "\n",
        "html_content = format_conversation(conversation)\n",
        "display(HTML(html_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCgLSxx0_6D9"
      },
      "source": [
        "# generation_evaluation_data.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "3kp3GMnm_a6d"
      },
      "outputs": [],
      "source": [
        "generation_evaluation_data = \"\"\"\n",
        "prompts:\n",
        "  get_evaluation_data:\n",
        "    prompt: |\n",
        "\n",
        "      Conversation\n",
        "      ============\n",
        "\n",
        "      {conversation}\n",
        "\n",
        "      Process Technical Explanation\n",
        "      =============================\n",
        "\n",
        "      {process_technical_explanation}\n",
        "\n",
        "      Task\n",
        "      ====\n",
        "\n",
        "      You are an evaluation system tasked with assessing the quality of conversation consists of assitant and user messages.\n",
        "      The assistant message starts with \"assistant:\" , user message starts with \"user:\".\n",
        "      The conversation contains user and assistant messages in chronological order from top to bottom.\n",
        "      The goal of conversation to fill definitions in \"Process Technical Explanation\".\n",
        "\n",
        "      - Metric: Completness - how the conversation has been ended.\n",
        "        -- Score = 1: if last assistant message contains information about possibility to proceed to Step 3 or Options Evaluation.\n",
        "        -- Score = 0: if any assistant message contains information about possibility to proceed to Step 3 or Options Evaluation , but it's not the last assistant message.\n",
        "        -- Score = -1: if non of assistant message contains information about possibility to proceed Step 3 or Options Evaluation.\n",
        "      - Metric: Smoothness - at what degree each user or assistant message follow from previous conversation\n",
        "        -- Check Each Message: For each message, determine whether it can be perfectly concluded from previous conversation.\n",
        "        -- Classify each message as either \"smooth\" or \"not smooth\".\n",
        "        -- Smoothness score: Calculate the percentage of messages that are smooth. Formula: (Number of \"smooth\" messages / Total number of messages) * 100\n",
        "      - Metric : User_specificity - Evaluates the degree to which user provides specific information on how to fulfill some definition.\n",
        "        Steps:\n",
        "        -- Check Each Message: For each user message, determine whether it is specific or not. A specific message provides clear, actionable information.\n",
        "        -- Classify each message as either \"specific\" or \"not specific\".\n",
        "        -- User_specificity score: Calculate accurately the percentage of user messages that are \"specific\". Formula: (Number of \"specific\" user messages / Total number of user messages) * 100\n",
        "      - Metric: Assistant_specificity - Evaluates the degree to which assistant provides specific information on how to fulfill some definition.\n",
        "        Steps:\n",
        "        -- Check Each Message: For each assistant message, determine whether it is specific or not. A specific message provides clear, actionable information.\n",
        "        -- Classify each message as either \"specific\" or \"not specific\".\n",
        "           Examples:\n",
        "           {Assistant_specificity_examples}\n",
        "        -- Assistant_specificity score: Calculate accurately the percentage of assistant messages that are \"specific\". Formula: (Number of \"specific\" assistant messages / Total number of assistant messages) * 100\n",
        "      - Metric : Assistant_technicality - Evaluates the degree to which user provides specific information on how to fulfill some definition.\n",
        "        Steps:\n",
        "        -- Check Each Message: For each assistant message, determine whether it contain technical information from \"Process Technical Explanation\" : Yes or Now. A technical message expose formats, formulas or calculations.\n",
        "        -- Classify each message as either \"technical\" or \"not technical\".\n",
        "           Examples:\n",
        "           {Assistant_technicality_examples}\n",
        "        -- Assistant_technicality score: Calculate accurately the percentage of assistant messages that are \"not technical\". Formula: (Number of \"not technical\" assistant messages / Total number of assistant messages) * 100\n",
        "\n",
        "      Think step-by-step and provide detailed explanation for score calculations.\n",
        "\n",
        "      Task Output: format the output as a JSON object as follows:\n",
        "      output:\n",
        "        metric: ?\n",
        "        score: ?\n",
        "\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FkULQIREmv2",
        "outputId": "4d8b67f0-8da2-4cee-b342-91e7f7667563"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3802"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Write the string to a text file\n",
        "import os\n",
        "with open(os.path.join(SYS_PROJECT_DIR, \"deoptima\", \"generation_evaluation_data.yaml\"), 'w') as file:\n",
        "    file.write(generation_evaluation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generation_learning_examples.json"
      ],
      "metadata": {
        "id": "EBzZ-EfktOgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Assistant_specificity_examples_dict = [\n",
        "{'assistant': \"Thank you! Now we have the problem, options, and criteria defined. Please compare each unique pair of criteria. For example, you can say 'Price is Equally Important to Performance'\",\n",
        "'classification': \"specific\"},\n",
        "{'assistant': \"Thank you for the comparisons! Now, please provide the next criteria comparison.\",\n",
        "'classification': \"not specific\"},\n",
        "]\n",
        "\n",
        "Assistant_technicality_examples_dict = [\n",
        "{\"assistant\": \"Thank you for your comparison! Now, please provide the remaining comparisons for the other pairs of criteria. Use the format ' is to/than '.\",\n",
        "\"classification\": \"technical\"},\n",
        "{\"assistant\": \"Thank you for your comparison! Now, please provide the remaining comparisons for the other pairs of criteria.\",\n",
        "\"classification\": \"not technical\"},\n",
        "]\n",
        "\n",
        "template = \"\"\"<assistant: \"{assistant}\", classification: \"{classification}\"/>\"\"\"\n",
        "\n",
        "Assistant_specificity_examples_str = []\n",
        "for i in Assistant_specificity_examples_dict:\n",
        "  i_str = template.format(assistant = i['assistant'], classification = i['classification'])\n",
        "  Assistant_specificity_examples_str.append(i_str)\n",
        "Assistant_technicality_examples_str = []\n",
        "for i in Assistant_technicality_examples_dict:\n",
        "  i_str = template.format(assistant = i['assistant'], classification = i['classification'])\n",
        "  Assistant_technicality_examples_str.append(i_str)\n",
        "\n",
        "generation_learning_examples = dict()\n",
        "generation_learning_examples['Assistant_specificity'] = Assistant_specificity_examples_str\n",
        "generation_learning_examples['Assistant_technicality'] = Assistant_technicality_examples_str\n"
      ],
      "metadata": {
        "id": "86geC7o7uPrB"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def save_to_json(obj, filepath):\n",
        "    with open(filepath, 'w') as file:\n",
        "        json.dump(obj, file, indent=4)\n",
        "\n",
        "save_to_json(generation_learning_examples, os.path.join(os.path.join(SYS_PROJECT_DIR, \"deoptima\", \"generation_learning_examples.json\")))"
      ],
      "metadata": {
        "id": "A-FFDC3G7Tw5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTbEtAQz-SGZ"
      },
      "source": [
        "# Sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "ydYCGxV_0RxJ",
        "outputId": "7972460b-bab2-4f8f-d57c-e7fbf5889a20"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-4b9a1d0b2ebd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "time.sleep(30000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-1BoKn6_wzO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "11eqV3lWnKESuFMDCbj7MVxCkFPF5xB0M",
      "authorship_tag": "ABX9TyNHPLIj4XN4SYK+oM4d6y6S",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}